{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6388f6cc-44f6-4d06-8eba-bc8a78c4cea7",
   "metadata": {},
   "source": [
    "### Langchain Tutorial: Chatbot with Persistent Memory\n",
    "\n",
    "https://python.langchain.com/docs/tutorials/chatbot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86842411-a142-4a6c-b38d-d4e5828ee666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages if not installed\n",
    "#!pip install -qU langchain-core langgraph>0.2.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "617d9da6-f48a-4da0-8c5d-508a78033c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API keys and environment variables\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# LangSmith can be used to debug / test / monitor AI Application \n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "if not os.environ.get(\"LANGSMITH_API_KEY\"):\n",
    "  os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter API key for LangSmith: \")\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50e02b0b-7c18-4d28-ac68-012364135ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e43205b-3ad5-4202-8584-78ec70a29998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in-memory checkpoint (Wrap with simple langraph application)\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0097ed8-bab8-442c-b11c-cfa76fe7cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add configuration\n",
    "\n",
    "# A thread is a unique ID or thread identifier assigned to each checkpoint saved by a checkpointer. \n",
    "# It contains the accumulated state of a sequence of runs. When a run is executed, the state of the \n",
    "# underlying graph of the assistant will be persisted to the thread\n",
    "\n",
    "# The state of a thread at a particular point in time is called a checkpoint. \n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0bbe7c1-febf-45b3-9c56-0bd16548c8a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Pubudu! Nice to meet you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# Invoke the application\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "query = \"Hi! My Name is Pubudu\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbec46fd-0c4f-4eb8-a6bb-45603e108932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Pubudu.\n"
     ]
    }
   ],
   "source": [
    "# Check if the chatbot has memory\n",
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "929a02c6-fcdd-4797-9b58-1deb0bc35319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "As an AI, I don't have access to personal information like your name. I don't retain memory of past interactions or personal details about users.\n",
      "\n",
      "If you'd like me to know your name for this conversation, you'll need to tell me!\n"
     ]
    }
   ],
   "source": [
    "# Change the 'thread_id' to start the conversation in fresh\n",
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "859de5f9-13ab-44be-b73a-44f0d39aade0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "It's great to meet you, Pubudu!\n"
     ]
    }
   ],
   "source": [
    "# Revert back to previous thread and it remembers the history\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "query1 = \"Write a sentence including my name\"\n",
    "\n",
    "input_messages = [HumanMessage(query1)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1e92c3c-1693-456e-8dfa-8d1dc570fddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Hi! My Name is Pubudu', additional_kwargs={}, response_metadata={}, id='9b5abdd2-933e-4130-8b1a-1d44ee14553c'), AIMessage(content='Hi Pubudu! Nice to meet you. How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--aafca1b0-cc75-48be-a40d-9b289c7fb2fc-0', usage_metadata={'input_tokens': 8, 'output_tokens': 410, 'total_tokens': 418, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 394}}), HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='a8181426-60de-4dcf-8740-6c91a96b3d24'), AIMessage(content='Your name is Pubudu.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--d738b28c-0d7e-442f-be27-97c23405b45e-0', usage_metadata={'input_tokens': 32, 'output_tokens': 44, 'total_tokens': 76, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 38}}), HumanMessage(content='Write a sentence including my name', additional_kwargs={}, response_metadata={}, id='c589a83e-e19d-4b3c-8419-1eb46c8677d7'), AIMessage(content=\"It's great to meet you, Pubudu!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--21409398-acf3-4801-8cb4-024b9b561145-0', usage_metadata={'input_tokens': 46, 'output_tokens': 47, 'total_tokens': 93, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 36}})]}\n"
     ]
    }
   ],
   "source": [
    "# View entire model output\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
