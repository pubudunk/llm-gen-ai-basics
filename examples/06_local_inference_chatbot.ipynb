{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84290037",
   "metadata": {},
   "source": [
    "### Langchain Tutorial: Inference LLM locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "248521cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the packages once\n",
    "# !pip install -U transformers accelerate langchain-core langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2f5e6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.51s/it]\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Create the pipeline\n",
    "# We use a small footprint model which can be loaded and invoked in a standard laptop\n",
    "# without GPU support. 'microsoft/phi-2'\n",
    "\n",
    "# Pipeline Info:\n",
    "# We use the tokenizer from the same model.\n",
    "# Model parameters can be tweeked. (temperature / top_k / top_p). We use values for\n",
    "# a typical general chatbot\n",
    "\n",
    "# Observe the output message that the running device does not have GPU support thus reverts to\n",
    "# cpu support only.\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"microsoft/phi-2\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "mdl = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "gen_pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=mdl,\n",
    "    tokenizer=tok,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "950acc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap with the huggingface\n",
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=gen_pipe)      # LLM wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a275b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Use ChatPromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a brief, helpful assistant.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm \n",
    "\n",
    "output = chain.invoke({\"input\": \"Give me 3 fancy Italian restaurant names.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a79de80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a brief, helpful assistant.\n",
      "Human: Give me 3 fancy Italian restaurant names.\n",
      "Assistant: Certainly! Here are 3 fancy Italian restaurant names: \n",
      "1. La Trattoria \n",
      "2. Il Ristorante \n",
      "3. Il Forno del Gusto\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
